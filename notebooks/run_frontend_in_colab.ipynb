{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally. Ensuring dependencies are installed...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# Add the src directory to the path so we can import search_frontend\n",
        "# Assuming the notebook is in 'notebooks/' and src is in '../src/'\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "src_path = os.path.join(project_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "# Also add the current directory just in case\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.append(os.getcwd())\n",
        "\n",
        "# Install dependencies if running locally and they might be missing\n",
        "if not IS_COLAB:\n",
        "    print(\"Running locally. Ensuring dependencies are installed...\")\n",
        "    !pip install -q nltk flask google-cloud-storage pandas requests gcsfs pyarrow\n",
        "else:\n",
        "    print(\"Running in Colab.\")\n",
        "    !pip install -q gcsfs pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCPDHP7zTQJZ",
        "outputId": "c75f4593-706c-4d2d-ddca-b3261fb845e2"
      },
      "outputs": [],
      "source": [
        "# download nltk stopwords\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAt6KT8xOgHH",
        "outputId": "0c67a9ce-b162-49b9-b614-8c6d895b9fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping GCS downgrade for local environment.\n"
          ]
        }
      ],
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "if IS_COLAB:\n",
        "    !pip install -q google-cloud-storage==1.43.0\n",
        "else:\n",
        "    print(\"Skipping GCS downgrade for local environment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-oKFly5jFLFn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not running in Colab, skipping Google Auth (assuming local credentials are set up).\n"
          ]
        }
      ],
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "# Only run this if in Colab\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab, skipping Google Auth (assuming local credentials are set up).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not in Colab. If you have id_to_title.pkl in bucket, download it manually or run this in Colab.\n"
          ]
        }
      ],
      "source": [
        "# Generate id_to_title.pkl from parquet files if missing\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from google.cloud import storage\n",
        "\n",
        "# Only run this if we are in Colab or have auth\n",
        "if IS_COLAB:\n",
        "    try:\n",
        "        bucket_name = 'yali-ir2025-bucket' # Hardcoded from config\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(bucket_name)\n",
        "        \n",
        "        # Check if id_to_title.pkl already exists in bucket\n",
        "        blob = bucket.blob('id_to_title.pkl')\n",
        "        if blob.exists():\n",
        "            print(\"id_to_title.pkl already exists in bucket. Downloading...\")\n",
        "            blob.download_to_filename('id_to_title.pkl')\n",
        "        else:\n",
        "            print(\"Generating id_to_title.pkl from parquet files...\")\n",
        "            # List parquet files\n",
        "            blobs = list(bucket.list_blobs(prefix='multistream'))\n",
        "            parquet_files = [b.name for b in blobs if b.name.endswith('.parquet')]\n",
        "            \n",
        "            id_to_title = {}\n",
        "            \n",
        "            for pq_file in parquet_files:\n",
        "                print(f\"Processing {pq_file}...\")\n",
        "                # Read parquet file directly from GCS\n",
        "                uri = f\"gs://{bucket_name}/{pq_file}\"\n",
        "                try:\n",
        "                    df = pd.read_parquet(uri, columns=['id', 'title'])\n",
        "                    for index, row in df.iterrows():\n",
        "                        id_to_title[str(row['id'])] = row['title']\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {pq_file}: {e}\")\n",
        "            \n",
        "            print(f\"Collected {len(id_to_title)} titles.\")\n",
        "            \n",
        "            # Save to local\n",
        "            with open('id_to_title.pkl', 'wb') as f:\n",
        "                pickle.dump(id_to_title, f)\n",
        "            \n",
        "            # Upload to bucket\n",
        "            print(\"Uploading id_to_title.pkl to bucket...\")\n",
        "            blob.upload_from_filename('id_to_title.pkl')\n",
        "            print(\"Done.\")\n",
        "            \n",
        "        # Move to data folder if it exists\n",
        "        if not os.path.exists('data'):\n",
        "            os.makedirs('data')\n",
        "        if os.path.exists('id_to_title.pkl'):\n",
        "            import shutil\n",
        "            shutil.copy('id_to_title.pkl', 'data/id_to_title.pkl')\n",
        "            print(\"Copied id_to_title.pkl to data/ folder.\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error in id_to_title generation: {e}\")\n",
        "else:\n",
        "    print(\"Not in Colab. If you have id_to_title.pkl in bucket, download it manually or run this in Colab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dW0y91OVu5J"
      },
      "source": [
        "# Run the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7opNkV6uRHIv"
      },
      "outputs": [],
      "source": [
        "# Import the frontend module\n",
        "# Ensure you have 'src' in your python path (see cell above)\n",
        "try:\n",
        "    import search_frontend as se\n",
        "except ImportError:\n",
        "    # Fallback if running from a different directory context\n",
        "    from src import search_frontend as se\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oTGXXYEXV5l8"
      },
      "outputs": [],
      "source": [
        "# uncomment the code below and execute to reload the module when you make\n",
        "# changes to search_frontend.py (after you upload again).\n",
        "# import importlib\n",
        "# importlib.reload(se)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "k7K7oFlfVhm5",
        "outputId": "8f31e64f-e631-4dca-a182-ea283cf9ecf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting server on port 8080...\n",
            " * Serving Flask app 'search_frontend'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8080\n",
            " * Running on http://192.168.7.14:8080\n",
            "Press CTRL+C to quit\n",
            "127.0.0.1 - - [02/Jan/2026 16:15:25] \"GET / HTTP/1.1\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local Server URL: http://127.0.0.1:8080\n",
            "Server is up and running!\n",
            "Test URL: http://127.0.0.1:8080/search_body?query=hello+world\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "import requests\n",
        "\n",
        "PORT = 8080\n",
        "\n",
        "def run_server():\n",
        "    # Run the app\n",
        "    # Note: use_reloader=False is important in notebooks/background threads\n",
        "    try:\n",
        "        se.app.run(host='0.0.0.0', port=PORT, debug=False, use_reloader=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Server failed to start: {e}\")\n",
        "\n",
        "# Start the server in a background thread\n",
        "print(f\"Starting server on port {PORT}...\")\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(3) \n",
        "\n",
        "# Check if server is up\n",
        "if IS_COLAB:\n",
        "    from google.colab.output import eval_js\n",
        "    server_url = eval_js(f\"google.colab.kernel.proxyPort({PORT})\")\n",
        "    print(f\"Colab Server URL: {server_url}\")\n",
        "else:\n",
        "    server_url = f\"http://127.0.0.1:{PORT}\"\n",
        "    print(f\"Local Server URL: {server_url}\")\n",
        "\n",
        "try:\n",
        "    # Simple health check\n",
        "    requests.get(server_url)\n",
        "    print(\"Server is up and running!\")\n",
        "except:\n",
        "    print(\"Warning: Server might not be reachable yet or failed to start.\")\n",
        "\n",
        "print(f\"Test URL: {server_url}/search_body?query=hello+world\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0MC_1nzDbi"
      },
      "source": [
        "# Testing your app\n",
        "\n",
        "Once your app is running you can query it. You can simply do that by clicking on the URL printed above (the one looking like https://XXXXX-5000-colab.googleusercontent.com/search?query=hello+world or by issuing an HTTP request through code (from colab).\n",
        "\n",
        "The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EM5ePrRHojbG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading queries from: c:\\Users\\User\\Desktop\\סמסטר א\\אחזור מידע\\פרויקט\\data\\queries_train.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to queries file\n",
        "# Try 'data/queries_train.json' (relative to project root) or just 'queries_train.json'\n",
        "queries_path = os.path.join(project_root, 'data', 'queries_train.json')\n",
        "if not os.path.exists(queries_path):\n",
        "    queries_path = 'queries_train.json' # Fallback\n",
        "\n",
        "print(f\"Loading queries from: {queries_path}\")\n",
        "with open(queries_path, 'rt') as f:\n",
        "  queries = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gWimZWCOy3Ei"
      },
      "outputs": [],
      "source": [
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    for i,doc_id in enumerate(predicted_list):\n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)\n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return round(sum(precisions)/len(precisions),3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "geHKyFB4xkBe"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(predicted_list) == 0:\n",
        "        return 0.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)\n",
        "def recall_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(true_set) < 1:\n",
        "        return 1.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)\n",
        "def f1_at_k(true_list, predicted_list, k):\n",
        "    p = precision_at_k(true_list, predicted_list, k)\n",
        "    r = recall_at_k(true_list, predicted_list, k)\n",
        "    if p == 0.0 or r == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p + 1.0/r), 3)\n",
        "def results_quality(true_list, predicted_list):\n",
        "    p5 = precision_at_k(true_list, predicted_list, 5)\n",
        "    f1_30 = f1_at_k(true_list, predicted_list, 30)\n",
        "    if p5 == 0.0 or f1_30 == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)\n",
        "\n",
        "assert precision_at_k(range(10), [1,2,3] , 2) == 1.0\n",
        "assert recall_at_k(   range(10), [10,5,3], 2) == 0.1\n",
        "assert precision_at_k(range(10), []      , 2) == 0.0\n",
        "assert precision_at_k([],        [1,2,3],  5) == 0.0\n",
        "assert recall_at_k(   [],        [10,5,3], 2) == 1.0\n",
        "assert recall_at_k(   range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       [],        [1,2,3],  5) == 0.0\n",
        "assert f1_at_k(       range(10), [],       2) == 0.0\n",
        "assert f1_at_k(       range(10), [0,1,2],  2) == 0.333\n",
        "assert f1_at_k(       range(50), range(5), 30) == 0.182\n",
        "assert f1_at_k(       range(50), range(10), 30) == 0.333\n",
        "assert f1_at_k(       range(50), range(30), 30) == 0.75\n",
        "assert results_quality(range(50), range(5))  == 0.308\n",
        "assert results_quality(range(50), range(10)) == 0.5\n",
        "assert results_quality(range(50), range(30)) == 0.857\n",
        "assert results_quality(range(50), [-1]*5 + list(range(5,30))) == 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dYmNTq9u0ChK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing against: http://127.0.0.1:8080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:15:34] \"GET /search_body?query=Mount+Everest+climbing+expeditions HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Mount Everest climbing expeditions | Duration: 9.441s | Quality: 0.198 | AP: 0.569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:15:51] \"GET /search_body?query=Great+Fire+of+London+1666 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Great Fire of London 1666 | Duration: 16.781s | Quality: 0.0 | AP: 0.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:15:59] \"GET /search_body?query=Nanotechnology+materials+science HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Nanotechnology materials science | Duration: 7.659s | Quality: 0.571 | AP: 0.704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:08] \"GET /search_body?query=Fossil+fuels+climate+change HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Fossil fuels climate change | Duration: 9.257s | Quality: 0.291 | AP: 0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:18] \"GET /search_body?query=DNA+double+helix+discovery HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: DNA double helix discovery | Duration: 9.741s | Quality: 0.311 | AP: 0.396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:28] \"GET /search_body?query=Printing+press+invention+Gutenberg HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Printing press invention Gutenberg | Duration: 10.777s | Quality: 0.425 | AP: 0.577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:38] \"GET /search_body?query=Ancient+Egypt+pyramids+pharaohs HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Ancient Egypt pyramids pharaohs | Duration: 9.482s | Quality: 0.377 | AP: 0.488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:47] \"GET /search_body?query=Gothic+literature+Mary+Shelley HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Gothic literature Mary Shelley | Duration: 9.426s | Quality: 0.236 | AP: 0.41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:16:55] \"GET /search_body?query=Robotics+automation+industry HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Robotics automation industry | Duration: 7.544s | Quality: 0.283 | AP: 0.375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:07] \"GET /search_body?query=Television+invention+broadcast+media HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Television invention broadcast media | Duration: 12.123s | Quality: 0.0 | AP: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:15] \"GET /search_body?query=Wright+brothers+first+flight HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Wright brothers first flight | Duration: 7.703s | Quality: 0.0 | AP: 0.079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:21] \"GET /search_body?query=Steam+locomotive+transportation+history HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Steam locomotive transportation history | Duration: 6.126s | Quality: 0.175 | AP: 0.379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:29] \"GET /search_body?query=Currency+history+gold+standard HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Currency history gold standard | Duration: 7.813s | Quality: 0.201 | AP: 0.246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:38] \"GET /search_body?query=Renaissance+art+Leonardo+da+Vinci HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Renaissance art Leonardo da Vinci | Duration: 9.583s | Quality: 0.457 | AP: 0.585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:48] \"GET /search_body?query=Shakespeare+plays+Elizabethan+theatre HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Shakespeare plays Elizabethan theatre | Duration: 10.296s | Quality: 0.188 | AP: 0.261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:17:57] \"GET /search_body?query=Solar+eclipse+astronomy+observation HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Solar eclipse astronomy observation | Duration: 8.317s | Quality: 0.198 | AP: 0.459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:06] \"GET /search_body?query=Renaissance+architecture+Florence+Italy HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Renaissance architecture Florence Italy | Duration: 8.786s | Quality: 0.25 | AP: 0.572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:11] \"GET /search_body?query=Impressionism+Monet+Renoir HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Impressionism Monet Renoir | Duration: 5.906s | Quality: 0.364 | AP: 0.44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:22] \"GET /search_body?query=Samurai+code+Bushido+Japan HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Samurai code Bushido Japan | Duration: 10.145s | Quality: 0.231 | AP: 0.466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:33] \"GET /search_body?query=Fossil+record+paleontology+evidence HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Fossil record paleontology evidence | Duration: 11.747s | Quality: 0.165 | AP: 0.317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:45] \"GET /search_body?query=Silk+Road+trade+cultural+exchange HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Silk Road trade cultural exchange | Duration: 12.114s | Quality: 0.165 | AP: 0.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:18:54] \"GET /search_body?query=Industrial+Revolution+steam+engines HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Industrial Revolution steam engines | Duration: 8.812s | Quality: 0.417 | AP: 0.566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:19:07] \"GET /search_body?query=Green+Revolution+agriculture+yield HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Green Revolution agriculture yield | Duration: 12.460s | Quality: 0.0 | AP: 0.04\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:19:20] \"GET /search_body?query=Quantum+computing+future+technology HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Quantum computing future technology | Duration: 13.532s | Quality: 0.368 | AP: 0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:19:40] \"GET /search_body?query=Viking+exploration+North+America HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Viking exploration North America | Duration: 19.591s | Quality: 0.229 | AP: 0.411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:19:50] \"GET /search_body?query=Roman+aqueducts+engineering+innovation HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Roman aqueducts engineering innovation | Duration: 10.432s | Quality: 0.202 | AP: 0.385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:19:56] \"GET /search_body?query=Coffee+history+Ethiopia+trade HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Coffee history Ethiopia trade | Duration: 5.964s | Quality: 0.447 | AP: 0.573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:20:02] \"GET /search_body?query=Stonehenge+prehistoric+monument HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Stonehenge prehistoric monument | Duration: 5.286s | Quality: 0.397 | AP: 0.565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:20:07] \"GET /search_body?query=Photography+invention+Daguerre HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Photography invention Daguerre | Duration: 5.856s | Quality: 0.381 | AP: 0.615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jan/2026 16:20:24] \"GET /search_body?query=Ballet+origins+France+Russia HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Ballet origins France Russia | Duration: 16.154s | Quality: 0.193 | AP: 0.255\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from time import time\n",
        "\n",
        "# Use the server_url defined in the previous cell\n",
        "# If it's not defined, fallback to localhost\n",
        "if 'server_url' not in locals():\n",
        "    server_url = 'http://127.0.0.1:8080'\n",
        "\n",
        "print(f\"Testing against: {server_url}\")\n",
        "\n",
        "qs_res = []\n",
        "for q, true_wids in queries.items():\n",
        "  duration, ap = None, None\n",
        "  t_start = time()\n",
        "  try:\n",
        "    # Note: Using /search_body for now as that's what we implemented\n",
        "    # The original code used /search. You can change this back later.\n",
        "    res = requests.get(server_url + '/search_body', {'query': q}, timeout=35)\n",
        "    duration = time() - t_start\n",
        "    if res.status_code == 200:\n",
        "      # The response is a list of (doc_id, title)\n",
        "      # We need just the doc_ids for evaluation\n",
        "      results = res.json()\n",
        "      pred_wids = [str(doc_id) for doc_id, title in results]\n",
        "      \n",
        "      # Calculate quality metrics\n",
        "      rq = results_quality(true_wids, pred_wids)\n",
        "      ap = average_precision(true_wids, pred_wids)\n",
        "      \n",
        "      print(f\"Query: {q} | Duration: {duration:.3f}s | Quality: {rq} | AP: {ap}\")\n",
        "    else:\n",
        "        print(f\"Query: {q} | Failed with status {res.status_code}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Query: {q} | Error: {e}\")\n",
        "\n",
        "  qs_res.append((q, duration, ap))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
