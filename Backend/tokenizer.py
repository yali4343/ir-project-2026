import re

RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
stopwords_frozen = frozenset(
    [
        "corp",
        "re",
        "is",
        "has",
        "mightn",
        "d",
        "ll",
        "y",
        "s",
        "t",
        "ve",
        "m",
        "o",
        "won",
        "now",
        "am",
        "an",
        "and",
        "are",
        "as",
        "at",
        "be",
        "by",
        "do",
        "go",
        "he",
        "if",
        "in",
        "me",
        "my",
        "no",
        "of",
        "on",
        "or",
        "so",
        "to",
        "up",
        "us",
        "we",
        "ad",
        "af",
        "ah",
        "ai",
        "am",
        "an",
        "as",
        "at",
        "aw",
        "ax",
        "ay",
        "ba",
        "be",
        "bi",
        "bo",
        "by",
        "ch",
        "do",
        "dy",
        "eh",
        "el",
        "em",
        "en",
        "er",
        "es",
        "et",
        "ex",
        "fa",
        "go",
        "ha",
        "he",
        "hi",
        "hm",
        "ho",
        "id",
        "if",
        "in",
        "is",
        "it",
        "jo",
        "ka",
        "ki",
        "la",
        "li",
        "lo",
        "ma",
        "me",
        "mi",
        "mm",
        "mo",
        "mu",
        "my",
        "na",
        "ne",
        "no",
        "nu",
        "od",
        "oe",
        "of",
        "oh",
        "oi",
        "ok",
        "om",
        "on",
        "op",
        "or",
        "os",
        "ow",
        "ox",
        "oy",
        "pa",
        "pe",
        "pi",
        "po",
        "qi",
        "re",
        "sh",
        "si",
        "so",
        "ta",
        "ti",
        "to",
        "uh",
        "um",
        "un",
        "up",
        "us",
        "ut",
        "we",
        "wo",
        "xi",
        "xu",
        "ya",
        "ye",
        "yo",
        "za",
    ]
)


def tokenize(text):
    """
    Tokenizes the input text by:
    1. Lowercasing.
    2. Extracting words using regex.
    3. Filtering out stopwords and short tokens.

    Args:
        text (str): Input text string.

    Returns:
        list: List of token strings.
    """
    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    return [token for token in list_of_tokens if token not in stopwords_frozen]
